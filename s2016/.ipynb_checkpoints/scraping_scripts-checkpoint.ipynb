{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports applicable to all scraping scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#', 'Season', 'Name', 'Team', \n",
    "            'G', 'PA', 'AB', 'H', '1B', '2B', \n",
    "            '3B', 'HR', 'R', 'RBI', 'BB', 'IBB', \n",
    "            'SO', 'HBP', 'SF', 'SH', 'GDP', 'SB', \n",
    "            'CS', 'AVG', 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter into url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=1&startDate=',\n",
    "           str(day),\n",
    "                '&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=22,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    sleep(1)\n",
    "    \n",
    "    #pull html table from first page\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        url_list[5] = str(p)\n",
    "        url = \"\".join(url_list)\n",
    "        browser.get(url)\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_9_02_to_10_01 = master_df\n",
    "subset_9_02_to_10_01.to_csv('results_9_02_to_10_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir():\n",
    "    if file.endswith('csv'):\n",
    "        master_df = master_df.append(pd.read_csv(file, index_col = 0))\n",
    "master_df.to_csv('results_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get headers\n",
    "url = \"http://www.fangraphs.com/leaderssplits.aspx?splitArr=7&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=2016-06-10&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=0\"\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser.get(url)   \n",
    "sleep(1)\n",
    "soup = BS(browser.page_source, \"lxml\")\n",
    "table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=7&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_home_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "geckodriver.log\n",
      "results_4_03_to_4_29.csv\n",
      "results_4_29_to_6_01.csv\n",
      "results_6_02_to_7_01.csv\n",
      "results_7_02_to_8_01.csv\n",
      "results_8_02_to_9_01.csv\n",
      "results_9_02_to_10_01.csv\n",
      "results_master.csv\n",
      "scraping_scripts.ipynb\n",
      "season_home_04_04_to_05_01.csv\n",
      "season_home_05_02_to_06_01.csv\n",
      "season_home_06_02_to_07_01.csv\n",
      "season_home_07_02_to_08_01.csv\n",
      "season_home_09_02_to_10_02.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data/hitting/'):\n",
    "    \n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_home'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_home_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Away\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=8&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_away_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_away'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_away_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lefties\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=1&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_lefties_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_lefties'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_lefties_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rightes\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=2&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_righties_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_righties'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_righties_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past 7 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 9, day = 25)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/week_09_09_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('week'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/week_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batted Balls Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "page = 0\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2016-04-03&endDate=',\n",
    "       str(day),\n",
    "        '&filter=&position=B&statType=player&autoPt=false&sort=17,1&pg=',\n",
    "       str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)\n",
    "table = None\n",
    "    \n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'GB/FB',\n",
    " 'LD%',\n",
    " 'GB%',\n",
    " 'FB%',\n",
    " 'IFFB%',\n",
    " 'HR/FB',\n",
    " 'IFH%',\n",
    " 'BUH%',\n",
    " 'Pull%',\n",
    " 'Cent%',\n",
    " 'Oppo%',\n",
    " 'Soft%',\n",
    " 'Med%',\n",
    " 'Hard%',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=17,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_batted_balls_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batted Balls 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'GB/FB',\n",
    " 'LD%',\n",
    " 'GB%',\n",
    " 'FB%',\n",
    " 'IFFB%',\n",
    " 'HR/FB',\n",
    " 'IFH%',\n",
    " 'BUH%',\n",
    " 'Pull%',\n",
    " 'Cent%',\n",
    " 'Oppo%',\n",
    " 'Soft%',\n",
    " 'Med%',\n",
    " 'Hard%',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 9, day = 25)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=17,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/past_7_batted_balls_09_09_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('past_7_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/past_7_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "page = 0\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=team&autoPt=false&sort=15,1&pg=',\n",
    "           str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)   \n",
    "table = None\n",
    "\n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'BB%',\n",
    " 'K%',\n",
    " 'BB/K',\n",
    " 'AVG',\n",
    " 'OBP',\n",
    " 'SLG',\n",
    " 'OPS',\n",
    " 'ISO',\n",
    " 'BABIP',\n",
    " 'wRC',\n",
    " 'wRAA',\n",
    " 'wOBA',\n",
    " 'wRC+',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 7, day = 3)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=team&autoPt=false&sort=15,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/team_season_07_03_to_09_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('team_season'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/team_season_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 7, day = 2)\n",
    "end = datetime.date(year = 2016, month = 9, day = 25)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=team&autoPt=false&sort=15,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/team_past7_07_09_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('team_past7'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/team_past7_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitching\n",
    "## Home Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'K/9',\n",
    " 'BB/9',\n",
    " 'K/BB',\n",
    " 'HR/9',\n",
    " 'K%',\n",
    " 'BB%',\n",
    " 'K-BB%',\n",
    " 'AVG',\n",
    " 'WHIP',\n",
    " 'BABIP',\n",
    " 'LOB%',\n",
    " 'FIP',\n",
    " 'xFIP',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=10&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=17,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/season_home_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('season_home'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/season_home_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=10&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=17,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/season_away_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('season_away'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/season_away_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past Three Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetime.date(year = 2016, month = 3, day = 17) + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 9, day = 2)\n",
    "end = datetime.date(year = 2016, month = 9, day = 16)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 17)),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=17,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 17))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 17))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/past_3games_09_19_to_10_03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('past_3games'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/past_3games_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batted Balls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2016-04-08&endDate=',\n",
    "       str(day),\n",
    "        '&filter=&position=P&statType=player&autoPt=true&sort=18,-1&pg=',\n",
    "       str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)   \n",
    "table = None\n",
    "\n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'GB/FB',\n",
    " 'LD%',\n",
    " 'GB%',\n",
    " 'FB%',\n",
    " 'IFFB%',\n",
    " 'HR/FB',\n",
    " 'IFH%',\n",
    " 'BUH%',\n",
    " 'Pull%',\n",
    " 'Cent%',\n",
    " 'Oppo%',\n",
    " 'Soft%',\n",
    " 'Med%',\n",
    " 'Hard%',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Failed to decode response from marionette\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-340-661121c43ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m            str(page)]\n\u001b[1;32m     21\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    238\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'alert'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Failed to decode response from marionette\n"
     ]
    }
   ],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 4, day = 3)\n",
    "end = datetime.date(year = 2016, month = 5, day = 1)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=18,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/season_batted_balls_09_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('season_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/season_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past Three Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 8, day = 2)\n",
    "end = datetime.date(year = 2016, month = 9, day = 16)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 17)),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=18,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 17))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")        \n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 17))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/past_3games_batted_balls_08_19_to_10_03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('past_3games_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/past_3games_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bullpen\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "       str(day),\n",
    "        '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "       str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)   \n",
    "table = None\n",
    "\n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'K/9',\n",
    " 'BB/9',\n",
    " 'K/BB',\n",
    " 'HR/9',\n",
    " 'K%',\n",
    " 'BB%',\n",
    " 'K-BB%',\n",
    " 'AVG',\n",
    " 'WHIP',\n",
    " 'BABIP',\n",
    " 'LOB%',\n",
    " 'FIP',\n",
    " 'xFIP',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 7, day = 2)\n",
    "end = datetime.date(year = 2016, month = 10, day = 2)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/bullpen_season_07_02_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('bullpen_season'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/bullpen_season_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past 7 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2016, month = 6, day = 2)\n",
    "end = datetime.date(year = 2016, month = 9, day = 25)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/bullpen_past7_06_09_to_10_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('bullpen_past7'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/bullpen_past7_master.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
