{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = ['#', 'Season', 'Name', 'Team', \n",
    "            'G', 'PA', 'AB', 'H', '1B', '2B', \n",
    "            '3B', 'HR', 'R', 'RBI', 'BB', 'IBB', \n",
    "            'SO', 'HBP', 'SF', 'SH', 'GDP', 'SB', \n",
    "            'CS', 'AVG', 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "# range is inclusive\n",
    "start_dict = {'year': 2013, 'month': 4, 'day': 9}\n",
    "end_dict = {'year': 2013, 'month': 5, 'day': 2}\n",
    "start = datetime.date(**start_dict)\n",
    "end = datetime.date(**end_dict)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# iterate through all days in range, and enter into url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=1&startDate=',\n",
    "           str(day),\n",
    "                '&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=22,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    sleep(1)\n",
    "    \n",
    "    #pull html table from first page\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        url_list[5] = str(p)\n",
    "        url = \"\".join(url_list)\n",
    "        browser.get(url)\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)\n",
    "filename = 'data/hitting/results_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/hitting/results_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "path = 'data/hitting/'\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('csv'):\n",
    "        master_df = master_df.append(pd.read_csv(path + file, index_col = 0))\n",
    "master_df.to_csv('results_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hitting\n",
    "### P7 Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get headers\n",
    "url = \"http://www.fangraphs.com/leaderssplits.aspx?splitArr=7&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=2016-06-10&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=0\"\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(url)   \n",
    "sleep(1)\n",
    "soup = BS(browser.page_source, \"lxml\")\n",
    "table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start_dict = {'year': 2017, 'month': 8, 'day': 2}\n",
    "end_dict = {'year': 2017, 'month': 9, 'day': 1}\n",
    "start = datetime.date(**start_dict)\n",
    "end = datetime.date(**end_dict)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        try:\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            sleep(1)\n",
    "            fg_pop = browser.find_elements_by_class_name('popup-button-container')\n",
    "            if fg_pop:\n",
    "                fg_pop[0].find_element_by_partial_link_text('No Thanks').click()\n",
    "                sleep(1)\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)\n",
    "        \n",
    "path = 'data/hitting/'\n",
    "filename = path + 'week_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('week'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/week_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P7 Batted Balls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'GB/FB',\n",
    " 'LD%',\n",
    " 'GB%',\n",
    " 'FB%',\n",
    " 'IFFB%',\n",
    " 'HR/FB',\n",
    " 'IFH%',\n",
    " 'BUH%',\n",
    " 'Pull%',\n",
    " 'Cent%',\n",
    " 'Oppo%',\n",
    " 'Soft%',\n",
    " 'Med%',\n",
    " 'Hard%',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start_dict = {'year': 2017, 'month': 9, 'day': 2}\n",
    "end_dict = {'year': 2017, 'month': 9, 'day': 23}\n",
    "start = datetime.date(**start_dict)\n",
    "end = datetime.date(**end_dict)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=17,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        try:\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            fg_pop = browser.find_elements_by_class_name('popup-button-container')\n",
    "            if fg_pop:\n",
    "                fg_pop[0].find_element_by_partial_link_text('No Thanks').click()\n",
    "                sleep(1)\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)\n",
    "        \n",
    "path = 'data/hitting/'\n",
    "filename = path + 'past_7_batted_balls_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'past_7_batted_balls_'\n",
    "path = 'data/hitting/'\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith(prefix):\n",
    "        master_df = master_df.append(pd.read_csv(path + file, index_col = 0))\n",
    "master_df.to_csv(path + prefix + 'master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitching\n",
    "### Past Three Starts (17 Days) Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'K/9',\n",
    " 'BB/9',\n",
    " 'K/BB',\n",
    " 'HR/9',\n",
    " 'K%',\n",
    " 'BB%',\n",
    " 'K-BB%',\n",
    " 'AVG',\n",
    " 'WHIP',\n",
    " 'BABIP',\n",
    " 'LOB%',\n",
    " 'FIP',\n",
    " 'xFIP',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start_dict = {'year': 2017, 'month': 9, 'day': 2}\n",
    "end_dict = {'year': 2017, 'month': 9, 'day': 14}\n",
    "start = datetime.date(**start_dict)\n",
    "end = datetime.date(**end_dict)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 17)),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=17,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 17))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):      \n",
    "        \n",
    "        try:\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            fg_pop = browser.find_elements_by_class_name('popup-button-container')\n",
    "            if fg_pop:\n",
    "                fg_pop[0].find_element_by_partial_link_text('No Thanks').click()\n",
    "                sleep(1)\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 17))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)\n",
    "        \n",
    "path = 'data/pitching/'\n",
    "filename = path + 'past_3games_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'past_3games_'\n",
    "path = 'data/pitching/'\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith(prefix):\n",
    "        master_df = master_df.append(pd.read_csv(path + file, index_col = 0))\n",
    "master_df.to_csv(path + prefix + 'master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past Three Starts (Batted Balls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2016-04-08&endDate=',\n",
    "       str(day),\n",
    "        '&filter=&position=P&statType=player&autoPt=true&sort=18,-1&pg=',\n",
    "       str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)   \n",
    "table = None\n",
    "\n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Season',\n",
       " 'Name',\n",
       " 'Team',\n",
       " 'IP',\n",
       " 'TBF',\n",
       " 'GB/FB',\n",
       " 'LD%',\n",
       " 'GB%',\n",
       " 'FB%',\n",
       " 'IFFB%',\n",
       " 'HR/FB',\n",
       " 'IFH%',\n",
       " 'BUH%',\n",
       " 'Pull%',\n",
       " 'Cent%',\n",
       " 'Oppo%',\n",
       " 'Soft%',\n",
       " 'Med%',\n",
       " 'Hard%',\n",
       " 'Date']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start_dict = {'year': 2017, 'month': 9, 'day': 2}\n",
    "end_dict = {'year': 2017, 'month': 9, 'day': 14}\n",
    "start = datetime.date(**start_dict)\n",
    "end = datetime.date(**end_dict)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 17)),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=18,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 17))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        try:\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            fg_pop = browser.find_elements_by_class_name('popup-button-container')\n",
    "            if fg_pop:\n",
    "                fg_pop[0].find_element_by_partial_link_text('No Thanks').click()\n",
    "                sleep(1)\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)        \n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 17))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)\n",
    "\n",
    "path = 'data/pitching/'\n",
    "filename = path + 'past_3games_batted_balls_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'past_3games_batted_balls_'\n",
    "path = 'data/pitching/'\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith(prefix):\n",
    "        master_df = master_df.append(pd.read_csv(path + file, index_col = 0))\n",
    "master_df.to_csv(path + prefix + 'master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bullpen\n",
    "### p7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'K/9',\n",
    " 'BB/9',\n",
    " 'K/BB',\n",
    " 'HR/9',\n",
    " 'K%',\n",
    " 'BB%',\n",
    " 'K-BB%',\n",
    " 'AVG',\n",
    " 'WHIP',\n",
    " 'BABIP',\n",
    " 'LOB%',\n",
    " 'FIP',\n",
    " 'xFIP',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start_dict = {'year': 2017, 'month': 9, 'day': 2}\n",
    "end_dict = {'year': 2017, 'month': 9, 'day': 23}\n",
    "start = datetime.date(**start_dict)\n",
    "end = datetime.date(**end_dict)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        try:\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            fg_pop = browser.find_elements_by_class_name('popup-button-container')\n",
    "            if fg_pop:\n",
    "                fg_pop[0].find_element_by_partial_link_text('No Thanks').click()\n",
    "                sleep(1)\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)        \n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)\n",
    "        \n",
    "path = 'data/pitching/'\n",
    "filename = path + 'bullpen_past7_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'bullpen_past7_'\n",
    "path = 'data/pitching/'\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith(prefix):\n",
    "        master_df = master_df.append(pd.read_csv(path + file, index_col = 0))\n",
    "master_df.to_csv(path + prefix + 'master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start_dict = {'year': 2017, 'month': 9, 'day': 2}\n",
    "end_dict = {'year': 2017, 'month': 10, 'day': 1}\n",
    "start = datetime.date(**start_dict)\n",
    "end = datetime.date(**end_dict)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "#binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "#browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser = webdriver.Chrome()\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=2017-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        try:\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)\n",
    "        except:\n",
    "            fg_pop = browser.find_elements_by_class_name('popup-button-container')\n",
    "            if fg_pop:\n",
    "                fg_pop[0].find_element_by_partial_link_text('No Thanks').click()\n",
    "                sleep(1)\n",
    "            browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "            sleep(1)        \n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)\n",
    "        \n",
    "path = 'data/pitching/'\n",
    "filename = path + 'bullpen_season_' + str(start_dict['month']) + \"_\"\n",
    "filename += str(start_dict['day']) + \"_to_\" + str(end_dict['month'])\n",
    "filename += \"_\" + str(end_dict['day']) + '.csv'\n",
    "master_df.to_csv(filename)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = 'bullpen_season_'\n",
    "path = 'data/pitching/'\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith(prefix):\n",
    "        master_df = master_df.append(pd.read_csv(path + file, index_col = 0))\n",
    "master_df.to_csv(path + prefix + 'master.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_root",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
