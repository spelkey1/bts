{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports applicable to all scraping scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from time import sleep\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#', 'Season', 'Name', 'Team', \n",
    "            'G', 'PA', 'AB', 'H', '1B', '2B', \n",
    "            '3B', 'HR', 'R', 'RBI', 'BB', 'IBB', \n",
    "            'SO', 'HBP', 'SF', 'SH', 'GDP', 'SB', \n",
    "            'CS', 'AVG', 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter into url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=1&startDate=',\n",
    "           str(day),\n",
    "                '&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=22,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    sleep(1)\n",
    "    \n",
    "    #pull html table from first page\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)        \n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/results_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir():\n",
    "    if file.endswith('csv'):\n",
    "        master_df = master_df.append(pd.read_csv(file, index_col = 0))\n",
    "master_df.to_csv('results_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get headers\n",
    "url = \"http://www.fangraphs.com/leaderssplits.aspx?splitArr=7&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=2016-06-10&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=0\"\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "browser.get(url)   \n",
    "sleep(1)\n",
    "soup = BS(browser.page_source, \"lxml\")\n",
    "table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'BB%',\n",
    " 'K%',\n",
    " 'BB/K',\n",
    " 'AVG',\n",
    " 'OBP',\n",
    " 'SLG',\n",
    " 'OPS',\n",
    " 'ISO',\n",
    " 'BABIP',\n",
    " 'wRC',\n",
    " 'wRAA',\n",
    " 'wOBA',\n",
    " 'wRC+',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6f8b392ec574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m            str(page)]\n\u001b[1;32m     21\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTemplate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstitute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparsed_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhttplib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=7&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)             \n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_home_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_home'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_home_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Away\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=8&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")        \n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_away_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_away'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_away_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lefties\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=1&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad interference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_lefties_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_lefties'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_lefties_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rightes\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=2&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad interference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")        \n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_righties_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_righties'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_righties_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past 7 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2014, 9, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.date(year = 2014, month = 9, day = 28) - datetime.timedelta(days = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 21)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=16,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad interference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")          \n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/week_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('week'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/week_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batted Balls Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "page = 0\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2016-04-03&endDate=',\n",
    "       str(day),\n",
    "        '&filter=&position=B&statType=player&autoPt=false&sort=17,1&pg=',\n",
    "       str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)\n",
    "table = None\n",
    "    \n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'GB/FB',\n",
    " 'LD%',\n",
    " 'GB%',\n",
    " 'FB%',\n",
    " 'IFFB%',\n",
    " 'HR/FB',\n",
    " 'IFH%',\n",
    " 'BUH%',\n",
    " 'Pull%',\n",
    " 'Cent%',\n",
    " 'Oppo%',\n",
    " 'Soft%',\n",
    " 'Med%',\n",
    " 'Hard%',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=17,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/season_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('season_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/season_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batted Balls 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'GB/FB',\n",
    " 'LD%',\n",
    " 'GB%',\n",
    " 'FB%',\n",
    " 'IFFB%',\n",
    " 'HR/FB',\n",
    " 'IFH%',\n",
    " 'BUH%',\n",
    " 'Pull%',\n",
    " 'Cent%',\n",
    " 'Oppo%',\n",
    " 'Soft%',\n",
    " 'Med%',\n",
    " 'Hard%',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 21)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=player&autoPt=false&sort=17,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/past_7_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('past_7_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/past_7_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "page = 0\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=team&autoPt=false&sort=15,1&pg=',\n",
    "           str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)   \n",
    "table = None\n",
    "\n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Team',\n",
    " 'PA',\n",
    " 'BB%',\n",
    " 'K%',\n",
    " 'BB/K',\n",
    " 'AVG',\n",
    " 'OBP',\n",
    " 'SLG',\n",
    " 'OPS',\n",
    " 'ISO',\n",
    " 'BABIP',\n",
    " 'wRC',\n",
    " 'wRAA',\n",
    " 'wOBA',\n",
    " 'wRC+',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=B&statType=team&autoPt=false&sort=15,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/team_season_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('team_season'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/team_season_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 21)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=B&statType=team&autoPt=false&sort=15,1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/hitting/team_past7_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/hitting/'):\n",
    "    if file.startswith('team_past7'):\n",
    "        master_df = master_df.append(pd.read_csv('data/hitting/'+file, index_col = 0))\n",
    "master_df.to_csv('data/hitting/team_past7_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitching\n",
    "## Home Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'K/9',\n",
    " 'BB/9',\n",
    " 'K/BB',\n",
    " 'HR/9',\n",
    " 'K%',\n",
    " 'BB%',\n",
    " 'K-BB%',\n",
    " 'AVG',\n",
    " 'WHIP',\n",
    " 'BABIP',\n",
    " 'LOB%',\n",
    " 'FIP',\n",
    " 'xFIP',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=9&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=17,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/season_home_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('season_home'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/season_home_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=10&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=17,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    \n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/season_away_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('season_away'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/season_away_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past Three Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2014, 9, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.date(year = 2014, month = 9, day = 28) - datetime.timedelta(days = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2014, 3, 13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.date(year = 2014, month = 3, day = 30) - datetime.timedelta(days = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 13)\n",
    "end = datetime.date(year = 2014, month = 9, day = 11)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 17)),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=17,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 17))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") \n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 17))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/past_3games_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('past_3games'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/past_3games_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batted Balls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2016-04-08&endDate=',\n",
    "       str(day),\n",
    "        '&filter=&position=P&statType=player&autoPt=true&sort=18,-1&pg=',\n",
    "       str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)   \n",
    "table = None\n",
    "\n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Name',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'GB/FB',\n",
    " 'LD%',\n",
    " 'GB%',\n",
    " 'FB%',\n",
    " 'IFFB%',\n",
    " 'HR/FB',\n",
    " 'IFH%',\n",
    " 'BUH%',\n",
    " 'Pull%',\n",
    " 'Cent%',\n",
    " 'Oppo%',\n",
    " 'Soft%',\n",
    " 'Med%',\n",
    " 'Hard%',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=18,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/season_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('season_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/season_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past Three Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 13)\n",
    "end = datetime.date(year = 2014, month = 9, day = 11)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=&strgroup=season&statgroup=3&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 17)),\n",
    "            '&filter=&position=P&statType=player&autoPt=false&sort=18,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 17))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")        \n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 17))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/past_3games_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('past_3games_batted_balls'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/past_3games_batted_balls_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bullpen\n",
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=2016-04-03&endDate=',\n",
    "       str(day),\n",
    "        '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "       str(page)]\n",
    "url = \"\".join(url_list)\n",
    "browser.get(url)   \n",
    "table = None\n",
    "\n",
    "#pull html table from first page\n",
    "while table == None:\n",
    "    soup = BS(browser.page_source, \"lxml\")\n",
    "    table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "    sleep(1)\n",
    "for tr in table.find('thead').findAll('tr'):\n",
    "    headers = [c.text for c in tr.findAll('th')]\n",
    "headers.append('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = ['#',\n",
    " 'Season',\n",
    " 'Team',\n",
    " 'IP',\n",
    " 'TBF',\n",
    " 'K/9',\n",
    " 'BB/9',\n",
    " 'K/BB',\n",
    " 'HR/9',\n",
    " 'K%',\n",
    " 'BB%',\n",
    " 'K-BB%',\n",
    " 'AVG',\n",
    " 'WHIP',\n",
    " 'BABIP',\n",
    " 'LOB%',\n",
    " 'FIP',\n",
    " 'xFIP',\n",
    " 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 28)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=2014-03-30&endDate=',\n",
    "           str(day),\n",
    "            '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day)\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #close stupid ad getting in the way\n",
    "    \n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        #click next button\n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day)\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/bullpen_season_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('bullpen_season'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/bullpen_season_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past 7 Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of days to iterate over\n",
    "start = datetime.date(year = 2014, month = 3, day = 30)\n",
    "end = datetime.date(year = 2014, month = 9, day = 21)\n",
    "date_range = []\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += datetime.timedelta(days = 1)\n",
    "\n",
    "# Create master dataframe and open firefox\n",
    "master_df = pd.DataFrame(columns = headers)\n",
    "binary = FirefoxBinary(r'C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe')\n",
    "browser = webdriver.Firefox(firefox_binary=binary)\n",
    "\n",
    "# iterate through all days in range, and enter in url\n",
    "for day in date_range:\n",
    "    page = 0\n",
    "    url_list = ['http://www.fangraphs.com/leaderssplits.aspx?splitArr=43&strgroup=season&statgroup=2&startDate=',\n",
    "           str(day),'&endDate=',str(day + datetime.timedelta(days = 7)),\n",
    "            '&filter=&position=P&statType=team&autoPt=false&sort=16,-1&pg=',\n",
    "           str(page)]\n",
    "    url = \"\".join(url_list)\n",
    "    browser.get(url)   \n",
    "    table = None\n",
    "    \n",
    "    #pull html table from first page\n",
    "    while table == None:\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        sleep(1)\n",
    "    body = table.find('tbody')\n",
    "    if body != None:\n",
    "        rows = body.findAll('tr')\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    #parses data and appends to master dataframe\n",
    "    data = []\n",
    "    for tr in rows:\n",
    "        cols = tr.findAll('td')\n",
    "        row_text = [c.text for c in cols]\n",
    "        row_text.append(day + datetime.timedelta(days = 7))\n",
    "        data.append(row_text)\n",
    "    data_df = pd.DataFrame(data, columns=headers)\n",
    "    master_df = master_df.append(data_df)\n",
    "    \n",
    "    #iterate through all pages for given day\n",
    "    page = int(soup.find('span', attrs = {'class':\"table-control-total\"}).text)\n",
    "    for p in range(1,page):\n",
    "        \n",
    "        #scroll to bottom of page to prevent ad intereference\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        browser.find_element_by_class_name('material-icons' and 'next').click()\n",
    "        sleep(1)\n",
    "        \n",
    "        #parses data and appends to master dataframe\n",
    "        data = []\n",
    "        soup = BS(browser.page_source, \"lxml\")\n",
    "        table = soup.find(attrs = {'class':\"table-splits\"})\n",
    "        body = table.find('tbody')\n",
    "        if body != None:\n",
    "            rows = body.findAll('tr')\n",
    "        else:\n",
    "            continue\n",
    "        for tr in rows:\n",
    "            cols = tr.findAll('td')\n",
    "            row_text = [c.text for c in cols]\n",
    "            row_text.append(day + datetime.timedelta(days = 7))\n",
    "            data.append(row_text)\n",
    "        data_df = pd.DataFrame(data, columns=headers)\n",
    "        master_df = master_df.append(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv('data/pitching/bullpen_past7_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame(columns = headers)\n",
    "for file in os.listdir('data/pitching/'):\n",
    "    if file.startswith('bullpen_past7'):\n",
    "        master_df = master_df.append(pd.read_csv('data/pitching/'+file, index_col = 0))\n",
    "master_df.to_csv('data/pitching/bullpen_past7_master.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
